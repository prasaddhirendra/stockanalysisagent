{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21ec207-71f0-48da-9e1c-f68201119bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2025 04:36:03 PM - Initiating enhanced search for query: 'the impact of generative AI on software development'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing search for query: 'the impact of generative AI on software development'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n",
      "08/25/2025 04:36:23 PM - Extracted string is not a valid URL: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Search Results ---\n",
      "No useful content found for the query after filtering. The tool might have encountered paywalls or non-parsable content.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import urllib.parse\n",
    "from crewai.tools import BaseTool\n",
    "from gnews import GNews\n",
    "from newspaper import Article\n",
    "\n",
    "# Configure logging for better feedback\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedSearchTool(BaseTool):\n",
    "    \"\"\"A tool to search the internet for news articles and extract their content.\"\"\"\n",
    "    \n",
    "    name: str = \"Enhanced Search\"\n",
    "    description: str = (\n",
    "        \"Searches the internet for recent information and extracts the full content of relevant articles. \"\n",
    "        \"Input should be a simple search query string.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Runs the search query, fetches articles, and returns their concatenated content.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query string.\n",
    "            \n",
    "        Returns:\n",
    "            str: The combined content of the fetched articles, or a message if no content is found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Initiating enhanced search for query: '{query}'\")\n",
    "            \n",
    "            gn = GNews(language='en', country='us', period='7d')\n",
    "            articles = gn.get_news(query)\n",
    "\n",
    "            if not articles:\n",
    "                return f\"No news articles found for the query: '{query}'.\"\n",
    "\n",
    "            full_content_list = []\n",
    "            max_articles_to_process = 5\n",
    "            processed_count = 0\n",
    "\n",
    "            for article_meta in articles:\n",
    "                if processed_count >= max_articles_to_process:\n",
    "                    logger.info(\"Reached maximum number of articles to process. Stopping.\")\n",
    "                    break\n",
    "\n",
    "                article_url_to_process = article_meta.get('url')\n",
    "                if not article_url_to_process:\n",
    "                    continue\n",
    "\n",
    "                if \"news.google.com/rss\" in article_url_to_process:\n",
    "                    try:\n",
    "                        parsed_url = urllib.parse.urlparse(article_url_to_process)\n",
    "                        query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "                        if 'oc' in query_params:\n",
    "                            potential_url = query_params['oc'][0]\n",
    "                            # New, more specific validation: check for a recognizable domain\n",
    "                            # A simple check for a period followed by 2-4 letters (or more)\n",
    "                            if any(tld in potential_url for tld in ['.com', '.org', '.net', '.io', '.co', '.ai']):\n",
    "                                article_url_to_process = potential_url\n",
    "                            else:\n",
    "                                logger.warning(f\"Extracted string is not a valid URL: {potential_url}\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            logger.warning(f\"Could not extract original URL from Google redirect link: {article_url_to_process}\")\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error parsing Google redirect URL: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Final check: Ensure the URL has a valid scheme\n",
    "                if not article_url_to_process.startswith(('http://', 'https://')):\n",
    "                    article_url_to_process = f\"https://{article_url_to_process}\"\n",
    "                    logger.info(f\"Adding 'https://' scheme to URL: {article_url_to_process}\")\n",
    "\n",
    "                try:\n",
    "                    news_article = Article(article_url_to_process)\n",
    "                    news_article.download()\n",
    "                    news_article.parse()\n",
    "                    \n",
    "                    content_text = news_article.text\n",
    "                    title = news_article.title\n",
    "                    \n",
    "                    if len(content_text) > 200: \n",
    "                        full_content_list.append(f\"Title: {title}\\nURL: {article_url_to_process}\\nContent:\\n{content_text}\")\n",
    "                        processed_count += 1\n",
    "                        logger.info(f\"Successfully processed article: '{title}' from {article_url_to_process}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Skipping article at {article_url_to_process} due to insufficient content.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error fetching article at {article_url_to_process}: {e}\")\n",
    "                \n",
    "                logger.info(\"=\" * 80)\n",
    "            \n",
    "            if not full_content_list:\n",
    "                return \"No useful content found for the query after filtering. The tool might have encountered paywalls or non-parsable content.\"\n",
    "\n",
    "            return \"\\n\\n---\\n\\n\".join(full_content_list)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search failed: {e}\")\n",
    "            return f\"Search failed: {e}\"\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    search_tool_instance = EnhancedSearchTool()\n",
    "    \n",
    "    query = \"latest technology news\"\n",
    "    print(f\"Executing search for query: '{query}'\")\n",
    "    search_results = search_tool_instance._run(query)\n",
    "    \n",
    "    print(\"\\n--- Search Results ---\")\n",
    "    print(search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (StockAnalysisAgent)",
   "language": "python",
   "name": "stockenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
